---
title: Process, Thread
categories: [Computer Science, Computer Architecture/Operating System]
tags: [] # TAG names should always be lowercase
---

## âœ… Process

> programs that are dispatched from the ready state and are scheduled in the CPU for execution. <br>
> ë©”ëª¨ë¦¬ ìƒì—ì„œ ì‹¤í–‰ì¤‘ì¸ í”„ë¡œê·¸ë¨ <br>

1ï¸âƒ£ **System call**: to create process <br>

ğŸ’¡ System call <https://soheeparklee.github.io/posts/OS-5systemcall/> <br>

2ï¸âƒ£ **Process has own address space(code, heap, stack)** <br>

- `Code`: memory(program command)
- `Data`: global variables, static variables, arrays
  - reset data: saved in data
  - not reset data: saved in bss
- `Heap`: dynamic allocation `new()`, `malloc()`
- `Stack`: local variable, parameter, return value (temporary)

<br>

3ï¸âƒ£ One process has each address space, <br>
and **cannot** access other process's variable or data <br>

<br>

4ï¸âƒ£ For another process to access other process's resources <br>
need to use **IPC(Inter Process Communication)** to communicate <br>
ğŸ’¡ IPC <https://soheeparklee.github.io/posts/OS-7IPC/> <br>

<br>

5ï¸âƒ£ One process has minimum one thread <br>

âœ”ï¸ **Two types of process**

- OS process
- user process

<br>

âœ”ï¸ **Disadvantages of process**

- ğŸ‘ğŸ» Long time to create
- as process has own resources and space
- ğŸ‘ğŸ» difficult to communicate among process(IPC)
- ğŸ‘ğŸ» process _context switching_ is inefficient, big overhead

## âœ… Thread

> **CPU ì‚¬ìš©ì˜ ê¸°ë³¸ ë‹¨ìœ„** <br>
> segment of a process <br>
> process = consisted of multiple threads <br>
> í”„ë¡œì„¸ìŠ¤ ì•ˆì—ì„œ ì‹¤í–‰ë˜ëŠ” ë‹¨ìœ„ <br>

âœ”ï¸ **Thread is consisted of...**

- threadID
- program counter
- register set, stack

<br>

1ï¸âƒ£ Only gets **individual stack** allocated <br>

- â­ï¸ share other areas with other thread(code, data, heap)
- â­ï¸ but has own stack
- when one thread changes one resource, other `sibling thread` can also see the change immediately

<br>

2ï¸âƒ£ when one process is created, minimum one thread is also created <br>

- no thread outside process

<br>

âœ”ï¸ **Advantages of thread**

- created to complement process
- ğŸ‘ğŸ» creation, termination fast
- ğŸ‘ğŸ» overhead â¬‡ï¸
- ğŸ‘ğŸ» communication among thread is easy and fast
  - share process memory, resource
  - threads can communicate without the help of kernel
- ğŸ‘ğŸ» smaller work unit than process
- ğŸ‘ğŸ» fast context switching

<br>

âœ”ï¸ **Disadvantages of thread**

- ğŸ‘ğŸ» when thread is created, use memory, CPU
- ğŸ‘ğŸ» if there is too much thread, `synchronization`, `resource sharing` problem might happen

![Screenshot 2024-07-25 at 11 30 17](https://github.com/user-attachments/assets/d8edfbe9-69a3-4be7-adc3-8452410f2561)

## â˜‘ï¸ Thread address space

- In multithreading, all thread in same process share
  - code
  - data
  - heap area
- Stack area is not shared
- Bc of resource sharing, problems such as synchronization can occur

âœ”ï¸ **private space**

- thread code space
- local variable for thread
- _stack_

âœ”ï¸ **shared space**

- data
- code
- heap

âœ”ï¸ **kernel stack**

<img width="326" alt="Screenshot 2024-09-08 at 00 59 04" src="https://github.com/user-attachments/assets/b7e3ab72-917c-421b-b48a-c5d2c20309b2">

<br>

> ğŸ’¡ **Which space has faster access speed? Stack ğŸ†š Heap** <br>
>
> > Stack has faster allocation, deallocation speed <br>

- **Stack** uses space that is already allocated
- Allocation in stack means changing pointer in already allocated space

  - ğŸ‘ğŸ» Simple CPU Instrucion, faster
  - ğŸ‘ğŸ» However, stack has very little space
  - ğŸ‘ğŸ» Cannot use stack for all thread

- **Heap** needs space to be allocated
- Allocation in heap means allocating new space caculating
  - required chunk,
  - current memory fragmentation...
  - ğŸ‘ğŸ» require more CPU Instruction

## Kernel level thread ğŸ†š User level thread

âœ”ï¸ **Kernel level thread**

> inside kernel

- managed by OS kernel
- OS handles the thread directly
- each kernel level thread has its own context
- ğŸ› ï¸ Java thread, POSIX thread on Linux

- ğŸ‘ğŸ» true parallelism
- ğŸ‘ğŸ» execution continuity
- ğŸ‘ğŸ» acess to system resources
- ğŸ‘ğŸ» more time to create, manage, context swithcing
- ğŸ‘ğŸ» more overhead on kernel

<img width="169" alt="Screenshot 2024-09-19 at 11 09 31" src="https://github.com/user-attachments/assets/149de8ce-6b6f-4ce3-8b74-b62e965af54c">

âœ”ï¸ **User level thread**

> outside kernel

- managed by user-level library
  - created, managed by thread library
- no intervention from OS kernel âŒ

- ğŸ‘ğŸ» faster than kernel level
- ğŸ‘ğŸ» no overhead of kernel thread
- ğŸ‘ğŸ» highly portable: can be implemented across various OS
- ğŸ‘ğŸ» limited use of multiprocessing
- ğŸ‘ğŸ» no scheduling priority based on overall system(do not know which thread will operate first)
- ğŸ› ï¸ fine control over threading

<img width="163" alt="Screenshot 2024-09-19 at 11 11 07" src="https://github.com/user-attachments/assets/896e5fe3-1cde-4195-aad2-63f58072dd06">

## Mode switch ğŸ†š Process switch

âœ”ï¸ **Mode switch**

> user level mode â¡ï¸ kernel mode

- use system stack

âœ”ï¸ **Process switch**

> context swithcing

- change currently running process, change to new process

## Process ğŸ†š Thread

- both are `units of work` on a computer
- process: has own address space, resources, independent
- thread: shares address space, resources with other threads
  - only has stack
  - operates within process

## â˜‘ï¸ Processing

- when lots of memory, resources are needed
- able the use to memory space and resource to be seperated

## â˜‘ï¸ Threading

- when there is a lot of work
- parallel threading
- able work to be done in order

## â˜‘ï¸ Thrashing

- when there is less SWAP space
- occurs when there is frequent page pansion
- CPU usage â¬†ï¸

## â­ï¸ Context Switching

> change process state to another state <br>

- to change to another process
- 1ï¸âƒ£ save current process context, state so that it can be restored later <br>
  - save current process `PCB(Process Control Block)`
- 2ï¸âƒ£ then load context or state of another process <br>
- ğŸ‘ğŸ» since each process has memory allocated, if heavy jobs are run, might have overhead problem.

#### â“ When does context switching occur?

âœ”ï¸ **Multitasking**

- processes take tuen according to OS scheduler
- each process is allocated CPU

âœ”ï¸ **Interrupt handling**

- when exception occurs, context switching occurs
- I/O request
- time slice expired: CPU use time expire
- for a child: create child process
- wait for an interrupt

âœ”ï¸ **User and kernel mode switching**

- change between user and kernel mode

## âœ… Multi Processing

> one program to be consisted of seveal processes <br>
> processes working in parrallel <br>
> increase power of a system <br>

- asymmetric multi processing
- symmetric multi processing

- ğŸ‘ğŸ» secure(solve memory invasion problem at OS level) <br>
- ğŸ‘ğŸ» save cost(if all CPUs share data in one disk)
- ğŸ‘ğŸ» as process has independent resource, does not affect other processes
- ğŸ‘ğŸ» each process has each memory, if lots of context switching â¡ï¸ overhead problem<br>
- ğŸ‘ğŸ» might be slow

> ğŸ’¡ **When should we use multi processing instead of multi threading?** <br>
>
> > - need distinction betweeen process memory <br>
> > - need process to have independent space address <br>

## âœ… Multi Threading

> normally, one thread in one process <br>
> multiple thread in one process <br>

- ğŸ‘ğŸ» multiple thread is made for increading computing **speed** of the system
- ğŸ‘ğŸ» **shares** address space, resources(unlike process) thus can **save time, resource** <br>
- ğŸ‘ğŸ» communication among thread is efficient, use heap
- ğŸ‘ğŸ» thread context switching is faster than process context switching

- ğŸ‘ğŸ» security <br>
- ğŸ‘ğŸ» if one thread breaks data space, other threads will be affected(since they share memory) <br>
- ğŸ‘ğŸ» need synchronization
- ğŸ‘ğŸ» deadlock
- ğŸ‘ğŸ» testing, debugging more difficult
- ğŸ’Š could be prevented by **Critical Section** method <br>

> ğŸ’¡ **When should we use multi threading instead of multi processing?** <br>
>
> > - to save resource allocation, as threads share resource <br>
> > - communication between thread is cost effective <br>
> > - ğŸ‘ğŸ» however, need to be cautiious about resource sharing! <br>

## â­ï¸ Critical section

> when one thread is chaning the value of shared data, <br>
> prevents from other thread accessing the data <br>
> only one thread at a time <br>

## âš ï¸ Synchronization

Synchronization <https://soheeparklee.github.io/posts/JAVA_multiThread/#-server-multithread> <br>

## ğŸ“Œ Semaphore, Mutex, Spinlock

Semaphore, Mutex, Spinlock <https://soheeparklee.github.io/posts/OS-11semapore/> <br>

## ğŸ“Œ Deadlock

Deadlock <https://soheeparklee.github.io/posts/OS-9deadlock/> <br>

## â­ï¸ Can the area created by a thread be accessed by other threads?

- YES
- need synchronization
- or shared vaiable among thread

## ğŸ’¡ Reference

Process/Thread <br>
<https://www.geeksforgeeks.org/difference-between-process-and-thread/> <br>

Multi Processing/Multi Thread <br>
<https://www.geeksforgeeks.org/difference-between-multiprocessing-and-multithreading/> <br>

Context Switching <br>
<https://www.geeksforgeeks.org/context-switch-in-operating-system/> <br>
